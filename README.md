# **Projeto: Pipeline de Qualidade do Ar com Apache Airflow**

Este projeto Ã© um pipeline ETL (Extract, Transform, Load) que coleta dados de qualidade do ar de todas as capitais brasileiras usando a API do OpenWeatherMap, transforma os dados e os carrega em um banco de dados PostgreSQL. O pipeline Ã© orquestrado pelo Apache Airflow, permitindo execuÃ§Ãµes automatizadas e monitoramento.

---

## **ğŸ“Œ SumÃ¡rio**
- [**Projeto: Pipeline de Qualidade do Ar com Apache Airflow**](#projeto-pipeline-de-qualidade-do-ar-com-apache-airflow)
  - [**ğŸ“Œ SumÃ¡rio**](#-sumÃ¡rio)
  - [**ğŸŒ VisÃ£o Geral**](#-visÃ£o-geral)
  - [**ğŸš€ Funcionalidades**](#-funcionalidades)
  - [**ğŸ›  Tecnologias Utilizadas**](#-tecnologias-utilizadas)
  - [**ğŸ“‚ Estrutura do Projeto**](#-estrutura-do-projeto)
  - [**ğŸ“‹ PrÃ©-requisitos**](#-prÃ©-requisitos)
  - [**âš™ ConfiguraÃ§Ã£o do Ambiente**](#-configuraÃ§Ã£o-do-ambiente)
  - [**â–¶ Executando o Projeto**](#-executando-o-projeto)
  - [**ğŸ“„ ExplicaÃ§Ã£o dos Arquivos**](#-explicaÃ§Ã£o-dos-arquivos)
  - [**ğŸ“ Contato**](#-contato)

---

## **ğŸŒ VisÃ£o Geral**
O projeto tem como objetivo monitorar a qualidade do ar em todas as capitais brasileiras. Ele coleta dados da API do OpenWeatherMap, transforma esses dados em um formato adequado e os armazena em um banco de dados PostgreSQL. O Apache Airflow Ã© usado para orquestrar o pipeline, permitindo execuÃ§Ãµes automatizadas e monitoramento.

---

## **ğŸš€ Funcionalidades**
- **ExtraÃ§Ã£o de Dados**: Coleta dados de qualidade do ar de todas as capitais brasileiras.
- **TransformaÃ§Ã£o de Dados**: Converte os dados brutos em um formato estruturado.
- **Carga de Dados**: Armazena os dados transformados em um banco de dados PostgreSQL.
- **OrquestraÃ§Ã£o**: Usa o Apache Airflow para agendar e monitorar o pipeline.
- **Logging**: Registra mensagens de log para facilitar a depuraÃ§Ã£o e o monitoramento.

---

## **ğŸ›  Tecnologias Utilizadas**
- **Python**: Linguagem de programaÃ§Ã£o principal.
- **Apache Airflow**: OrquestraÃ§Ã£o do pipeline.
- **PostgreSQL**: Armazenamento dos dados.
- **Docker**: ContÃªinerizaÃ§Ã£o do banco de dados e do Airflow.
- **API do OpenWeatherMap**: Fonte dos dados de qualidade do ar.
- **Pandas**: ManipulaÃ§Ã£o de dados.
- **Psycopg2**: ConexÃ£o com o PostgreSQL.
- **Requests**: RequisiÃ§Ãµes HTTP para a API.

---

## **ğŸ“‚ Estrutura do Projeto**
```
air_quality_pipeline/
â”‚â”€â”€ .env                  # VariÃ¡veis de ambiente
â”‚â”€â”€ .gitignore            # Arquivos e diretÃ³rios ignorados pelo Git
â”‚â”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o do PostgreSQL e Airflow
â”‚â”€â”€ requirements.txt      # DependÃªncias do projeto
â”‚â”€â”€ README.md             # DocumentaÃ§Ã£o do projeto
â”‚â”€â”€ main.py               # Script principal do pipeline
â”‚â”€â”€ extraction.py         # ExtraÃ§Ã£o de dados da API
â”‚â”€â”€ transform.py          # TransformaÃ§Ã£o dos dados
â”‚â”€â”€ load.py               # Carga dos dados no PostgreSQL
â”‚â”€â”€ db.py                 # ConexÃ£o com o PostgreSQL
â”‚â”€â”€ logs/                 # Arquivos de log
â”‚â”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ etl.py        # DAG do Apache Airflow
```

---

## **ğŸ“‹ PrÃ©-requisitos**
- **Docker**: Para rodar o PostgreSQL e o Apache Airflow.
- **Python 3.8+**: Para executar o pipeline.
- **Conta no OpenWeatherMap**: Para obter uma chave de API.

---

## **âš™ ConfiguraÃ§Ã£o do Ambiente**
1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/seu-usuario/air_quality_pipeline.git
   cd air_quality_pipeline
   ```

2. **Crie um arquivo `.env`**:
   - Renomeie o arquivo `.env.example` para `.env`.
   - Adicione sua chave de API do OpenWeatherMap e as credenciais do banco de dados.

3. **Instale as dependÃªncias**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Inicie o PostgreSQL e o Airflow**:
   ```bash
   docker-compose up -d
   ```

---

## **â–¶ Executando o Projeto**
1. **Execute o pipeline manualmente**:
   ```bash
   python main.py
   ```

2. **Acesse o Apache Airflow**:
   - Abra o navegador e acesse [http://localhost:8080](http://localhost:8080).
   - A DAG `air_quality_etl` estarÃ¡ disponÃ­vel para execuÃ§Ã£o.

3. **Verifique os logs**:
   - Os logs sÃ£o armazenados na pasta `logs/`.

---

## **ğŸ“„ ExplicaÃ§Ã£o dos Arquivos**
- **`.env`**: Armazena variÃ¡veis de ambiente, como chave de API e credenciais do banco de dados.
- **`docker-compose.yml`**: Configura o PostgreSQL e o Apache Airflow em contÃªineres Docker.
- **`main.py`**: Script principal que executa o pipeline ETL.
- **`extract.py`**: Extrai dados da API do OpenWeatherMap.
- **`transform.py`**: Transforma os dados brutos em um formato estruturado.
- **`load.py`**: Carrega os dados transformados no PostgreSQL.
- **`db.py`**: Gerencia a conexÃ£o com o banco de dados.
- **`airflow/dags/etl.py`**: Define a DAG do Apache Airflow para orquestrar o pipeline.

---

## **ğŸ“ Contato**
- **Linkedin**: [Giovana Hoffmann](www.linkedin.com/in/giovana-hoffmann-a53987255)

