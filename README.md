# **Projeto: Pipeline de Qualidade do Ar com Apache Airflow**

Este projeto Ã© um pipeline ETL (Extract, Transform, Load) que coleta dados de qualidade do ar de todas as capitais brasileiras usando a API do [OpenWeatherMap](https://openweathermap.org/api/air-pollution), transforma os dados e os carrega em um banco de dados PostgreSQL. O pipeline Ã© orquestrado pelo Apache Airflow, permitindo execuÃ§Ãµes automatizadas e monitoramento.

---

## **ðŸ“Œ SumÃ¡rio**
- [**Projeto: Pipeline de Qualidade do Ar com Apache Airflow**](#projeto-pipeline-de-qualidade-do-ar-com-apache-airflow)
  - [**ðŸ“Œ SumÃ¡rio**](#-sumÃ¡rio)
  - [**ðŸŒ VisÃ£o Geral**](#-visÃ£o-geral)
  - [**ðŸš€ Funcionalidades**](#-funcionalidades)
  - [**ðŸ›  Tecnologias Utilizadas**](#-tecnologias-utilizadas)
  - [**ðŸ“‚ Estrutura do Projeto**](#-estrutura-do-projeto)
  - [**ðŸ“ Arquitetura**](#-arquitetura)
  - [**ðŸ“‹ PrÃ©-requisitos**](#-prÃ©-requisitos)
  - [**âš™ ConfiguraÃ§Ã£o do Ambiente**](#-configuraÃ§Ã£o-do-ambiente)
  - [**â–¶ Executando o Projeto**](#-executando-o-projeto)
  - [**ðŸ“„ ExplicaÃ§Ã£o dos Arquivos**](#-explicaÃ§Ã£o-dos-arquivos)
  - [**ðŸ“ž Contato**](#-contato)

---

## **ðŸŒ VisÃ£o Geral**
O projeto tem como objetivo monitorar a qualidade do ar em todas as capitais brasileiras. Ele coleta dados da API do OpenWeatherMap, transforma esses dados em um formato adequado e os armazena em um banco de dados PostgreSQL. O Apache Airflow Ã© usado para orquestrar o pipeline, permitindo execuÃ§Ãµes automatizadas e monitoramento.

---

## **ðŸš€ Funcionalidades**
- **ExtraÃ§Ã£o de Dados**: Coleta dados de qualidade do ar de todas as capitais brasileiras.
- **TransformaÃ§Ã£o de Dados**: Converte os dados brutos em um formato estruturado.
- **Carga de Dados**: Armazena os dados transformados em um banco de dados PostgreSQL.
- **OrquestraÃ§Ã£o**: Usa o Apache Airflow para agendar e monitorar o pipeline.
- **Logging**: Registra mensagens de log para facilitar a depuraÃ§Ã£o e o monitoramento.

---

## **ðŸ›  Tecnologias Utilizadas**
- **Python**: Linguagem de programaÃ§Ã£o principal.
- **Apache Airflow**: OrquestraÃ§Ã£o do pipeline.
- **PostgreSQL**: Armazenamento dos dados.
- **Docker**: ContÃªinerizaÃ§Ã£o do banco de dados e do Airflow.
- **API do OpenWeatherMap**: Fonte dos dados de qualidade do ar.
- **Pandas**: ManipulaÃ§Ã£o de dados.
- **Psycopg2**: ConexÃ£o com o PostgreSQL.
- **Requests**: RequisiÃ§Ãµes HTTP para a API.

---

## **ðŸ“‚ Estrutura do Projeto**
```
air_quality_pipeline/
â”‚â”€â”€ .env                  # VariÃ¡veis de ambiente
â”‚â”€â”€ .gitignore            # Arquivos e diretÃ³rios ignorados pelo Git
â”‚â”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o do PostgreSQL e Airflow
â”‚â”€â”€ requirements.txt      # DependÃªncias do projeto
â”‚â”€â”€ README.md             # DocumentaÃ§Ã£o do projeto
â”‚â”€â”€ main.py               # Script principal do pipeline
â”‚â”€â”€ extraction.py         # ExtraÃ§Ã£o de dados da API
â”‚â”€â”€ transform.py          # TransformaÃ§Ã£o dos dados
â”‚â”€â”€ load.py               # Carga dos dados no PostgreSQL
â”‚â”€â”€ db.py                 # ConexÃ£o com o PostgreSQL
â”‚â”€â”€ utils.py              # ConfiguraÃ§Ã£o do load_dotenv e logging
â”‚â”€â”€ logs/                 # Arquivos de log
â”‚â”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ etl.py        # DAG do Apache Airflow
|â”€â”€ air_quality_dashboard.pbix
```
---
## **ðŸ“ Arquitetura**
```mermaid
graph TD
    A[OpenWeather API] --> B[Extract]
    B --> C[Transform]
    C --> D[Load]
    D --> E[(PostgreSQL)]
    E --> F[Airflow]
    F --> G[VisualizaÃ§Ã£o]
```
---

## **ðŸ“‹ PrÃ©-requisitos**
- **Docker**: Para rodar o PostgreSQL e o Apache Airflow.
- **Python 3.8+**: Para executar o pipeline.
- **Conta no OpenWeatherMap**: Para obter uma chave de API.

---

## **âš™ ConfiguraÃ§Ã£o do Ambiente**
1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/seu-usuario/air_quality_pipeline.git
   cd air_quality_pipeline
   ```

2. **Crie um arquivo `.env`**:
   - Renomeie o arquivo `.env-example` para `.env`.
   - Adicione sua chave de API do OpenWeatherMap e as credenciais do banco de dados.

3. **Instale as dependÃªncias**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Inicie o PostgreSQL e o Airflow**:
   ```bash
   docker-compose up -d
   ```

---

## **â–¶ Executando o Projeto**
   1. Via Airflow (recomendado):
      - Acesse http://localhost:8080
      - Ative a DAG air_quality_etl

   2. Manual:
      ```bash
      python main.py
      ```

Acessando os Dados
   - PGAdmin: http://localhost:5050
      - Credenciais definidas no .env

   - Consulta Direta:
      ```sql
      SELECT city, air_quality_index 
      FROM air_quality 
      WHERE date = CURRENT_DATE;
      ```
Verifique os logs:
   - Os logs sÃ£o armazenados na pasta `logs/`.

---

## **ðŸ“„ ExplicaÃ§Ã£o dos Arquivos**
- **`.env`**: Armazena variÃ¡veis de ambiente, como chave de API e credenciais do banco de dados.
- **`docker-compose.yml`**: Configura o PostgreSQL e o Apache Airflow em contÃªineres Docker.
- **`main.py`**: Script principal que executa o pipeline ETL.
- **`extract.py`**: Extrai dados da API do OpenWeatherMap.
- **`transform.py`**: Transforma os dados brutos em um formato estruturado.
- **`load.py`**: Carrega os dados transformados no PostgreSQL.
- **`db.py`**: Gerencia a conexÃ£o com o banco de dados.
- **`airflow/dags/etl.py`**: Define a DAG do Apache Airflow para orquestrar o pipeline.
- **`utils.py`**: ContÃ©m a configuraÃ§Ã£o do load_dotenv e logging.

---

## **ðŸ“ž Contato**
- **Linkedin**: [Giovana Hoffmann](www.linkedin.com/in/giovana-hoffmann-a53987255)

